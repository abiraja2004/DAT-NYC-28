{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision Trees\n",
    "\n",
    "*Adapted from Chapter 8 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Motivation:** Why are we learning about decision trees?\n",
    "\n",
    "- Useful for both regression and classification problems\n",
    "- Widely used\n",
    "- Basis for more sophisticated models\n",
    "- Have a different way of \"thinking\" than the other models we have studied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1: Regression trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Baseball player salary data:\n",
    "\n",
    "- **Years** (x-axis): number of years playing in the major leagues\n",
    "- **Hits** (y-axis): number of hits in the previous year\n",
    "- **Salary** (color): low salary is blue/green, high salary is red/yellow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Salary data](images/salary_color.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Group exercise:\n",
    "\n",
    "- The data above is our **training data**.\n",
    "- We want to build a model that predicts the Salary of **future players** based on Years and Hits.\n",
    "- We are going to \"segment\" the feature space into regions, and then use the **mean Salary in each region** as the predicted Salary for future players.\n",
    "- Intuitively, you want to **maximize** the similarity (or \"homogeneity\") within a given region, and **minimize** the similarity between different regions.\n",
    "\n",
    "Rules for segmenting:\n",
    "\n",
    "- You can only use **straight lines**, drawn one at a time.\n",
    "- Your line must either be **vertical or horizontal**.\n",
    "- Your line **stops** when it hits an existing line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Salary regions](images/salary_regions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Above are the regions created by a computer:\n",
    "\n",
    "- $R_1$: players with **less than 5 years** of experience, mean Salary of **\\$166,000 **\n",
    "- $R_2$: players with **5 or more years** of experience and **less than 118 hits**, mean Salary of **\\$403,000 **\n",
    "- $R_3$: players with **5 or more years** of experience and **118 hits or more**, mean Salary of **\\$846,000 **\n",
    "\n",
    "**Note:** Years and Hits are both integers, but the convention is to use the **midpoint** between adjacent values to label a split.\n",
    "\n",
    "These regions are used to make predictions on **out-of-sample data**. Thus, there are only three possible predictions! (Is this different from how **linear regression** makes predictions?)\n",
    "\n",
    "Below is the equivalent regression tree:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Salary tree](images/salary_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The first split is **Years < 4.5**, thus that split goes at the top of the tree. When a splitting rule is **True**, you follow the left branch. When a splitting rule is **False**, you follow the right branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For players in the **left branch**, the mean Salary is \\$166,000, thus you label it with that value. (Salary has been divided by 1000 and log-transformed to 5.11.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For players in the **right branch**, there is a further split on **Hits < 117.5**, dividing players into two more Salary regions: \\$403,000 (transformed to 6.00), and \\$846,000 (transformed to 6.74)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Salary tree annotated](images/salary_tree_annotated.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**What does this tree tell you about your data?**\n",
    "\n",
    "- Years is the most important factor determining Salary, with a lower number of Years corresponding to a lower Salary.\n",
    "- For a player with a lower number of Years, Hits is not an important factor determining Salary.\n",
    "- For a player with a higher number of Years, Hits is an important factor determining Salary, with a greater number of Hits corresponding to a higher Salary.\n",
    "\n",
    "**Question:** What do you like and dislike about decision trees so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Building a regression tree by hand\n",
    "\n",
    "Your **training data** is a tiny dataset of [used vehicle sale prices](https://raw.githubusercontent.com/justmarkham/DAT7/master/data/vehicles_train.csv). Your goal is to **predict price** for testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Read the data into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Explore the data by sorting, plotting, or split-apply-combine (aka `group_by`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Decide which feature is the most important predictor, and use that to create your first splitting rule.\n",
    "    - Only binary splits are allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- After making your first split, split your DataFrame into two parts, and then explore each part to figure out what other splits to make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Stop making splits once you are convinced that it strikes a good balance between underfitting and overfitting.\n",
    "    - Your goal is to build a model that generalizes well.\n",
    "    - You are allowed to split on the same variable multiple times!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Draw your tree, labeling the leaves with the mean price for the observations in that region.\n",
    "    - Make sure nothing is backwards: You follow the **left branch** if the rule is true, and the **right branch** if the rule is false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does a computer build a regression tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Ideal approach:** Consider every possible partition of the feature space (computationally infeasible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**\"Good enough\" approach:** recursive binary splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Begin at the top of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For **every feature**, examine **every possible cutpoint**, and choose the feature and cutpoint such that the resulting tree has the lowest possible mean squared error (MSE). Make that split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Examine the two resulting regions, and again make a **single split** (in one of the regions) to minimize the MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Keep repeating step 3 until a **stopping criterion** is met:\n",
    "    - maximum tree depth (maximum number of splits required to arrive at a leaf)\n",
    "    - minimum number of observations in a leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Demo: Choosing the ideal cutpoint for a given feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# vehicle data\n",
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT7/master/data/vehicles_train.csv'\n",
    "train = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22000</td>\n",
       "      <td>2012</td>\n",
       "      <td>13000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9500</td>\n",
       "      <td>2009</td>\n",
       "      <td>78000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000</td>\n",
       "      <td>2007</td>\n",
       "      <td>47000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4000</td>\n",
       "      <td>2006</td>\n",
       "      <td>124000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000</td>\n",
       "      <td>2004</td>\n",
       "      <td>177000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000</td>\n",
       "      <td>2004</td>\n",
       "      <td>209000</td>\n",
       "      <td>4</td>\n",
       "      <td>truck</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>138000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1900</td>\n",
       "      <td>2003</td>\n",
       "      <td>160000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2500</td>\n",
       "      <td>2003</td>\n",
       "      <td>190000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5000</td>\n",
       "      <td>2001</td>\n",
       "      <td>62000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1800</td>\n",
       "      <td>1999</td>\n",
       "      <td>163000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1300</td>\n",
       "      <td>1997</td>\n",
       "      <td>138000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  year   miles  doors  vtype   prediction\n",
       "0   22000  2012   13000      2    car  6571.428571\n",
       "1   14000  2010   30000      2    car  6571.428571\n",
       "2   13000  2010   73500      4    car  6571.428571\n",
       "3    9500  2009   78000      4    car  6571.428571\n",
       "4    9000  2007   47000      4    car  6571.428571\n",
       "5    4000  2006  124000      2    car  6571.428571\n",
       "6    3000  2004  177000      4    car  6571.428571\n",
       "7    2000  2004  209000      4  truck  6571.428571\n",
       "8    3000  2003  138000      2    car  6571.428571\n",
       "9    1900  2003  160000      4    car  6571.428571\n",
       "10   2500  2003  190000      2  truck  6571.428571\n",
       "11   5000  2001   62000      4    car  6571.428571\n",
       "12   1800  1999  163000      2  truck  6571.428571\n",
       "13   1300  1997  138000      4    car  6571.428571"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before splitting anything, just predict the mean of the entire dataset\n",
    "train['prediction'] = train.price.mean()\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5936.9819859959835"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate RMSE for those predictions\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "np.sqrt(metrics.mean_squared_error(train.price, train.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define a function that calculates the RMSE for a given split of miles\n",
    "def mileage_split(miles):\n",
    "    lower_mileage_price = train[train.miles < miles].price.mean()\n",
    "    higher_mileage_price = train[train.miles >= miles].price.mean()\n",
    "    train['prediction'] = np.where(train.miles < miles, lower_mileage_price, higher_mileage_price)\n",
    "    return np.sqrt(metrics.mean_squared_error(train.price, train.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3984.09174254\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22000</td>\n",
       "      <td>2012</td>\n",
       "      <td>13000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>15000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>15000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9500</td>\n",
       "      <td>2009</td>\n",
       "      <td>78000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000</td>\n",
       "      <td>2007</td>\n",
       "      <td>47000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>15000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4000</td>\n",
       "      <td>2006</td>\n",
       "      <td>124000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000</td>\n",
       "      <td>2004</td>\n",
       "      <td>177000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000</td>\n",
       "      <td>2004</td>\n",
       "      <td>209000</td>\n",
       "      <td>4</td>\n",
       "      <td>truck</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>138000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1900</td>\n",
       "      <td>2003</td>\n",
       "      <td>160000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2500</td>\n",
       "      <td>2003</td>\n",
       "      <td>190000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5000</td>\n",
       "      <td>2001</td>\n",
       "      <td>62000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1800</td>\n",
       "      <td>1999</td>\n",
       "      <td>163000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1300</td>\n",
       "      <td>1997</td>\n",
       "      <td>138000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  year   miles  doors  vtype    prediction\n",
       "0   22000  2012   13000      2    car  15000.000000\n",
       "1   14000  2010   30000      2    car  15000.000000\n",
       "2   13000  2010   73500      4    car   4272.727273\n",
       "3    9500  2009   78000      4    car   4272.727273\n",
       "4    9000  2007   47000      4    car  15000.000000\n",
       "5    4000  2006  124000      2    car   4272.727273\n",
       "6    3000  2004  177000      4    car   4272.727273\n",
       "7    2000  2004  209000      4  truck   4272.727273\n",
       "8    3000  2003  138000      2    car   4272.727273\n",
       "9    1900  2003  160000      4    car   4272.727273\n",
       "10   2500  2003  190000      2  truck   4272.727273\n",
       "11   5000  2001   62000      4    car   4272.727273\n",
       "12   1800  1999  163000      2  truck   4272.727273\n",
       "13   1300  1997  138000      4    car   4272.727273"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate RMSE for tree which splits on miles < 50000\n",
    "print 'RMSE:', mileage_split(50000)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3530.14653008\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22000</td>\n",
       "      <td>2012</td>\n",
       "      <td>13000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>12083.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>12083.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>12083.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9500</td>\n",
       "      <td>2009</td>\n",
       "      <td>78000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>12083.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000</td>\n",
       "      <td>2007</td>\n",
       "      <td>47000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>12083.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4000</td>\n",
       "      <td>2006</td>\n",
       "      <td>124000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>2437.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000</td>\n",
       "      <td>2004</td>\n",
       "      <td>177000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>2437.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000</td>\n",
       "      <td>2004</td>\n",
       "      <td>209000</td>\n",
       "      <td>4</td>\n",
       "      <td>truck</td>\n",
       "      <td>2437.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>138000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>2437.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1900</td>\n",
       "      <td>2003</td>\n",
       "      <td>160000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>2437.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2500</td>\n",
       "      <td>2003</td>\n",
       "      <td>190000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "      <td>2437.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5000</td>\n",
       "      <td>2001</td>\n",
       "      <td>62000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>12083.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1800</td>\n",
       "      <td>1999</td>\n",
       "      <td>163000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "      <td>2437.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1300</td>\n",
       "      <td>1997</td>\n",
       "      <td>138000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>2437.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  year   miles  doors  vtype    prediction\n",
       "0   22000  2012   13000      2    car  12083.333333\n",
       "1   14000  2010   30000      2    car  12083.333333\n",
       "2   13000  2010   73500      4    car  12083.333333\n",
       "3    9500  2009   78000      4    car  12083.333333\n",
       "4    9000  2007   47000      4    car  12083.333333\n",
       "5    4000  2006  124000      2    car   2437.500000\n",
       "6    3000  2004  177000      4    car   2437.500000\n",
       "7    2000  2004  209000      4  truck   2437.500000\n",
       "8    3000  2003  138000      2    car   2437.500000\n",
       "9    1900  2003  160000      4    car   2437.500000\n",
       "10   2500  2003  190000      2  truck   2437.500000\n",
       "11   5000  2001   62000      4    car  12083.333333\n",
       "12   1800  1999  163000      2  truck   2437.500000\n",
       "13   1300  1997  138000      4    car   2437.500000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate RMSE for tree which splits on miles < 100000\n",
    "print 'RMSE:', mileage_split(100000)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1ed29d68>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEPCAYAAAB7rQKTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ed20550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check all possible mileage splits\n",
    "mileage_range = range(train.miles.min(), train.miles.max(), 1000)\n",
    "RMSE = [mileage_split(miles) for miles in mileage_range]\n",
    "\n",
    "# plot mileage cutpoint (x-axis) versus RMSE (y-axis)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(mileage_range, RMSE)\n",
    "plt.xlabel('Mileage cutpoint')\n",
    "plt.ylabel('RMSE (lower is better)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Recap:** Before every split, this process is repeated for every feature, and the feature and cutpoint that produces the lowest MSE is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a regression tree in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# encode car as 0 and truck as 1\n",
    "train['vtype'] = train.vtype.map({'car':0, 'truck':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define X and y\n",
    "feature_cols = ['year', 'miles', 'doors', 'vtype']\n",
    "X = train[feature_cols]\n",
    "y = train.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a DecisionTreeRegressor (with random_state=1)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "treereg = DecisionTreeRegressor(random_state=1)\n",
    "treereg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3107.1428571428573"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use leave-one-out cross-validation (LOOCV) to estimate the RMSE for this model\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(treereg, X, y, cv=14, scoring='mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What happens when we grow a tree too deep?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Left: Regression tree for Salary **grown deeper**\n",
    "- Right: Comparison of the **training, testing, and cross-validation errors** for trees with different numbers of leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Salary tree grown deep](images/salary_tree_deep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The **training error** continues to go down as the tree size increases (due to overfitting), but the lowest **cross-validation error** occurs for a tree with 3 leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tuning a regression tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try to reduce the RMSE by tuning the **max_depth** parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4050.1443001442999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try different values one-by-one\n",
    "treereg = DecisionTreeRegressor(max_depth=1, random_state=1)\n",
    "scores = cross_val_score(treereg, X, y, cv=14, scoring='mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or, we could write a loop to try a range of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1ef6d2b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEQCAYAAACugzM1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xaeee550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list of values to try\n",
    "max_depth_range = range(1, 8)\n",
    "\n",
    "# list to store the average RMSE for each value of max_depth\n",
    "RMSE_scores = []\n",
    "\n",
    "# use LOOCV with each value of max_depth\n",
    "for depth in max_depth_range:\n",
    "    treereg = DecisionTreeRegressor(max_depth=depth, random_state=1)\n",
    "    MSE_scores = cross_val_score(treereg, X, y, cv=14, scoring='mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n",
    "\n",
    "# plot max_depth (x-axis) versus RMSE (y-axis)\n",
    "plt.plot(max_depth_range, RMSE_scores)\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('RMSE (lower is better)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=3, max_features=None,\n",
       "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_depth=3 was best, so fit a tree using that parameter\n",
    "treereg = DecisionTreeRegressor(max_depth=3, random_state=1)\n",
    "treereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>year</td>\n",
       "      <td>0.798744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>miles</td>\n",
       "      <td>0.201256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doors</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vtype</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature  importance\n",
       "0    year    0.798744\n",
       "1   miles    0.201256\n",
       "2   doors    0.000000\n",
       "3   vtype    0.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Gini importance\" of each feature: the (normalized) total reduction of error brought by that feature\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treereg.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating a tree diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create a GraphViz file\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(treereg, out_file='tree_vehicles.dot', feature_names=feature_cols)\n",
    "\n",
    "# At the command line, run this to convert to PNG:\n",
    "# dot -Tpng tree_vehicles.dot -o tree_vehicles.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Tree for vehicle data](images/tree_vehicles.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reading the internal nodes:\n",
    "\n",
    "- **samples:** number of observations in that node before splitting\n",
    "- **mse:** MSE calculated by comparing the actual response values in that node against the mean response value in that node\n",
    "- **rule:** rule used to split that node (go left if true, go right if false)\n",
    "\n",
    "Reading the leaves:\n",
    "\n",
    "- **samples:** number of observations in that node\n",
    "- **value:** mean response value in that node\n",
    "- **mse:** MSE calculated by comparing the actual response values in that node against \"value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Making predictions for the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>130000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6000</td>\n",
       "      <td>2005</td>\n",
       "      <td>82500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12000</td>\n",
       "      <td>2010</td>\n",
       "      <td>60000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  year   miles  doors  vtype\n",
       "0   3000  2003  130000      4      1\n",
       "1   6000  2005   82500      4      0\n",
       "2  12000  2010   60000      2      0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the testing data\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT7/master/data/vehicles_test.csv'\n",
    "test = pd.read_csv(url)\n",
    "test['vtype'] = test.vtype.map({'car':0, 'truck':1})\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Question:** Using the tree diagram above, what predictions will the model make for each observation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4000.,   5000.,  13500.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use fitted model to make predictions on testing data\n",
    "X_test = test[feature_cols]\n",
    "y_test = test.price\n",
    "y_pred = treereg.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1190.2380714238084"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate RMSE\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7937.2539331937714"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate RMSE for your own tree!\n",
    "y_test = [3000, 6000, 12000]\n",
    "y_pred = [0, 0, 0]\n",
    "from sklearn import metrics\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 2: Classification trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example:** Predict whether Barack Obama or Hillary Clinton will win the Democratic primary in a particular county in 2008:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Obama-Clinton decision tree](images/obama_clinton_tree.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Questions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What are the observations? How many observations are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What is the response variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What are the features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What is the most predictive feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Why does the tree split on high school graduation rate twice in a row?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What is the class prediction for the following county: 15% African-American, 90% high school graduation rate, located in the \n",
    "South, high poverty, high population density?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What is the predicted probability for that same county?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Comparing regression trees and classification trees\n",
    "\n",
    "|regression trees|classification trees|\n",
    "|---|---|\n",
    "|predict a continuous response|predict a categorical response|\n",
    "|predict using mean response of each leaf|predict using most commonly occuring class of each leaf|\n",
    "|splits are chosen to minimize MSE|splits are chosen to minimize Gini index (discussed below)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Splitting criteria for classification trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Common options for the splitting criteria:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **classification error rate:** fraction of training observations in a region that don't belong to the most common class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Gini impurity:** measure of how often a randomly chosen element from the set would be incorrectly labeled if it were  randomly labeled according to the distribution of labels in the subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example of classification error rate\n",
    "\n",
    "Pretend we are predicting whether someone buys an iPhone or an Android:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- At a particular node, there are **25 observations** (phone buyers), of whom **10 bought iPhones and 15 bought Androids**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since the majority class is **Android**, that's our prediction for all 25 observations, and thus the classification error rate is **10/25 = 40%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our goal in making splits is to **reduce the classification error rate**. Let's try splitting on gender:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Males:** 2 iPhones and 12 Androids, thus the predicted class is Android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Females:** 8 iPhones and 3 Androids, thus the predicted class is iPhone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Classification error rate after this split would be **5/25 = 20%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Compare that with a split on age:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **30 or younger:** 4 iPhones and 8 Androids, thus the predicted class is Android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **31 or older:** 6 iPhones and 7 Androids, thus the predicted class is Android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Classification error rate after this split would be **10/25 = 40%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The decision tree algorithm will try **every possible split across all features**, and choose the split that **reduces the error rate the most.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example of Gini impurity\n",
    "\n",
    "Calculate Gini impurity before making a split:\n",
    "\n",
    "$$1 - \\left(\\frac {iPhone} {Total}\\right)^2 - \\left(\\frac {Android} {Total}\\right)^2 = 1 - \\left(\\frac {10} {25}\\right)^2 - \\left(\\frac {15} {25}\\right)^2 = 0.48$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **maximum value** of Gini impurity is 0.5, and occurs when the classes are perfectly balanced in a node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **minimum value** of Gini impurity is 0, and occurs when there is only one class represented in a node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A node with a lower Gini impurity score is said to be more \"pure\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluating the split on **gender** using Gini index:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{Males: } 1 - \\left(\\frac {2} {14}\\right)^2 - \\left(\\frac {12} {14}\\right)^2 = 0.24$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{Females: } 1 - \\left(\\frac {8} {11}\\right)^2 - \\left(\\frac {3} {11}\\right)^2 = 0.40$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{Weighted Average: }  0.24 \\left(\\frac {14} {25}\\right) + 0.40 \\left(\\frac {11} {25}\\right) = 0.31$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluating the split on **age** using Gini impurity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{30 or younger: } 1 - \\left(\\frac {4} {12}\\right)^2 - \\left(\\frac {8} {12}\\right)^2 = 0.44$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{31 or older: } 1 - \\left(\\frac {6} {13}\\right)^2 - \\left(\\frac {7} {13}\\right)^2 = 0.50$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{Weighted Average: } 0.44 \\left(\\frac {12} {25}\\right) + 0.50 \\left(\\frac {13} {25}\\right) = 0.47$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Again, the decision tree algorithm will try **every possible split**, and will choose the split that **reduces the Gini impurity (and thus increases the \"node purity\") the most.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparing classification error rate and Gini impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Gini impurity is generally preferred because it will make splits that **increase node purity**, even if that split does not change the classification error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Node purity is important because we're interested in the **class proportions** in each region, since that's how we calculate the **predicted probability** of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- scikit-learn's default splitting criteria for classification trees is Gini impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note: There is another common splitting criteria called **cross-entropy**. It's numerically similar to Gini impurity, but slower to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a classification tree in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We'll build a classification tree using the Titanic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "7            8         0       3   \n",
       "8            9         1       3   \n",
       "9           10         1       2   \n",
       "\n",
       "                                                Name     Sex  Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male   22      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female   38      1   \n",
       "2                             Heikkinen, Miss. Laina  female   26      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female   35      1   \n",
       "4                           Allen, Mr. William Henry    male   35      0   \n",
       "5                                   Moran, Mr. James    male  NaN      0   \n",
       "6                            McCarthy, Mr. Timothy J    male   54      0   \n",
       "7                     Palsson, Master. Gosta Leonard    male    2      3   \n",
       "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female   27      0   \n",
       "9                Nasser, Mrs. Nicholas (Adele Achem)  female   14      1   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  \n",
       "5      0            330877   8.4583   NaN        Q  \n",
       "6      0             17463  51.8625   E46        S  \n",
       "7      1            349909  21.0750   NaN        S  \n",
       "8      2            347742  11.1333   NaN        S  \n",
       "9      0            237736  30.0708   NaN        C  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT7/master/data/titanic.csv'\n",
    "titanic = pd.read_csv(url)\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What special handling do we need to apply (if any) to the following columns?\n",
    "\n",
    "- **Survived:** 1=survived, 0=passed away (response variable)\n",
    "- **Pclass:** 1=first class, 2=second class, 3=third class\n",
    "    - What will happen if the tree splits on this feature?\n",
    "- **Sex:** male or female\n",
    "- **Age:** numeric value\n",
    "- **Embarked:** C or Q or S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>1</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>1</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>1</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "7            8         0       3   \n",
       "8            9         1       3   \n",
       "9           10         1       2   \n",
       "\n",
       "                                                Name  Sex        Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    1  22.000000      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0  38.000000      1   \n",
       "2                             Heikkinen, Miss. Laina    0  26.000000      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    0  35.000000      1   \n",
       "4                           Allen, Mr. William Henry    1  35.000000      0   \n",
       "5                                   Moran, Mr. James    1  29.699118      0   \n",
       "6                            McCarthy, Mr. Timothy J    1  54.000000      0   \n",
       "7                     Palsson, Master. Gosta Leonard    1   2.000000      3   \n",
       "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)    0  27.000000      0   \n",
       "9                Nasser, Mrs. Nicholas (Adele Achem)    0  14.000000      1   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  Embarked_Q  Embarked_S  \n",
       "0      0         A/5 21171   7.2500   NaN        S           0           1  \n",
       "1      0          PC 17599  71.2833   C85        C           0           0  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S           0           1  \n",
       "3      0            113803  53.1000  C123        S           0           1  \n",
       "4      0            373450   8.0500   NaN        S           0           1  \n",
       "5      0            330877   8.4583   NaN        Q           1           0  \n",
       "6      0             17463  51.8625   E46        S           0           1  \n",
       "7      1            349909  21.0750   NaN        S           0           1  \n",
       "8      2            347742  11.1333   NaN        S           0           1  \n",
       "9      0            237736  30.0708   NaN        C           0           0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode female as 0 and male as 1\n",
    "titanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\n",
    "\n",
    "# fill in the missing values for age with the mean age\n",
    "titanic.Age.fillna(titanic.Age.mean(), inplace=True)\n",
    "\n",
    "# create three dummy variables, drop the first dummy variable, and store the two remaining columns as a DataFrame\n",
    "embarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked').iloc[:, 1:]\n",
    "\n",
    "# concatenate the two dummy variable columns onto the original DataFrame\n",
    "titanic = pd.concat([titanic, embarked_dummies], axis=1)\n",
    "\n",
    "# print the updated DataFrame\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define X and y\n",
    "feature_cols = ['Pclass', 'Sex', 'Age', 'Embarked_Q', 'Embarked_S']\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit a classification tree with max_depth=3 on all data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "treeclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create a GraphViz file\n",
    "export_graphviz(treeclf, out_file='tree_titanic.dot', feature_names=feature_cols)\n",
    "\n",
    "# At the command line, run this to convert to PNG:\n",
    "# dot -Tpng tree_titanic.dot -o tree_titanic.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Tree for Titanic data](images/tree_titanic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice the split in the bottom right: the **same class** is predicted in both of its leaves. That split didn't affect the **classification error rate**, though it did increase the **node purity**, which is important because it increases the accuracy of our predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pclass</td>\n",
       "      <td>0.242664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0.655584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Age</td>\n",
       "      <td>0.064494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Embarked_Q</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Embarked_S</td>\n",
       "      <td>0.037258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  importance\n",
       "0      Pclass    0.242664\n",
       "1         Sex    0.655584\n",
       "2         Age    0.064494\n",
       "3  Embarked_Q    0.000000\n",
       "4  Embarked_S    0.037258"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the feature importances\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treeclf.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 3: Comparing decision trees with other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Advantages of decision trees:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Can be used for regression or classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Can be displayed graphically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Highly interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Can be specified as a series of rules, and more closely approximate human decision-making than other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Prediction is fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Features don't need scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Automatically learns feature interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Tends to ignore irrelevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Non-parametric (will outperform linear models if relationship between features and response is highly non-linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Trees versus linear models](images/tree_vs_linear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Disadvantages of decision trees:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Performance is (generally) not competitive with the best supervised learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Can easily overfit the training data (tuning is required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Small variations in the data can result in a completely different tree (high variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recursive binary splitting makes \"locally optimal\" decisions that may not result in a globally optimal tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Doesn't tend to work well if the classes are highly unbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Doesn't tend to work well with very small datasets"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
